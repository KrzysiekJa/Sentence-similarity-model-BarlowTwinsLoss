import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator


fsize = 15
tsize = 13
tdir = 'in'
major = 5.0
minor = 3.0
style = 'default'
lwidth = 0.5
lhandle = 3.0

plt.style.use(style)
plt.rcParams.update(plt.rcParamsDefault)
#plt.rcParams['text.usetex'] = True
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = fsize
plt.rcParams['legend.fontsize'] = tsize
plt.rcParams['xtick.direction'] = tdir
plt.rcParams['ytick.direction'] = tdir
plt.rcParams['xtick.major.size'] = major
plt.rcParams['xtick.minor.size'] = minor
plt.rcParams['ytick.major.size'] = major
plt.rcParams['ytick.minor.size'] = minor
plt.rcParams['axes.linewidth'] = lwidth
plt.rcParams['legend.handlelength'] = lhandle


########################################################################
# English
########################################################################
# bert-base-uncased
########################################################################
# title = 'BERT-base-uncased'
# train_0_01 = [0.6670863812934525, 0.7279276521899394, 0.7521322658828504, 0.7694349361328888, 0.7784602350525042, 0.7811061279783081]
# validation_0_01 = [0.7724997537508943 ,0.7933758909335401 ,0.8040932880094456 ,0.8074276337268831 ,0.810817984821049 ,0.812024127834295]
# test_0_01 = 0.7594756489046671
# train_0_005 = [0.6782729681792927, 0.7276760038208191, 0.7520256042465717, 0.7665433580224204, 0.7767568474645611, 0.7793812999842847]
# validation_0_005 = [0.7841141265347802 ,0.8028004472595911 ,0.8051851464141943 ,0.8058985668353588 ,0.8082816519386372 ,0.8089667130808331]
# test_0_005 = 0.7499771084578993
# train_0_001 = [0.6295178463759806, 0.7164170105010886, 0.7439169495333177, 0.7585925254980911, 0.7682028384124757, 0.7706709560269178]
# validation_0_001 = [0.7647324494582103 ,0.7864160185020919 ,0.7976775166779387 ,0.7970589237243507 ,0.8015285015419501 ,0.8013391455393872]
# test_0_001 = 0.7530739129462626
# train_0_0005 = [0.6585387741469894, 0.7344295089448608, 0.7481653150302372, 0.763060853248611, 0.7718778580978425, 0.7754874374917793]
# validation_0_0005 = [0.7796803385808362 ,0.7988387363405186 ,0.800053948922922 ,0.8027205308343331 ,0.8040517310412648 ,0.8049082725576401]
# test_0_0005 = 0.7565474507901706
# train_0_0001 = [0.6662209552534816, 0.7269999250196131, 0.7476775707469635, 0.7628557633725177, 0.770138746315303, 0.774274483169599]
# validation_0_0001 = [0.7734572437355118 ,0.795097514897452 ,0.8024075306183283 ,0.8016611333060786 ,0.8048844540358647 ,0.8055636182056307]
# test_0_0001 = 0.7468513979945739
# validation_neural = [0.8768652698350727 ,0.8795016915809777 ,0.8811073670648685 ,0.8823251365874557 ,0.8848721774888393 ,0.8848877166454567] # spearmanr
# test_pearson_neural = 0.8447226582406805
# test_spearmanr_neural = 0.830548903825659

########################################################################
# bert-large-uncased
########################################################################
# title = 'BERT-large-uncased'
# train_0_01 = [0.3999860858905723, 0.1597188637223978, -0.02102328024764908, 0.047740869796194685, 0.11975678271474609, 0.1915015142197596]
# validation_0_01 = [0.3217473353165915 ,0.3203192716331857 ,0.2547408025354958 ,0.1920777697805262 ,0.2303793350388756 ,0.2765513041090314]
# test_0_01 = 0.237601446684278
# train_0_005 = [0.422186923070396, 0.5116502781258778, 0.3963250654024163, 0.48018139104775054, 0.47947527991243155, 0.48398645888372627]
# validation_0_005 = [0.609776384183033 ,0.5977247204983323 ,0.5215804031916045 ,0.5986427387525192 ,0.5927938242110199 ,0.6004907455437857]
# test_0_005 = 0.47373802139437254
# train_0_001 = [0.6090953952094409, 0.596145987199234, 0.6999225492950739, 0.7104836403741716, 0.7328757361948954, 0.7356189195761665]
# validation_0_001 = [0.4353897118861285 ,0.6294615002343985 ,0.665083618825158 ,0.7335370720580614 ,0.7419104728544282 ,0.7412318300373352]
# test_0_001 = 0.7019013384896126
# train_0_0005 = [0.41395359262518416, 0.6007667579368258, 0.6422142497194905, 0.6957332116628696, 0.7175762612141069, 0.7321280823227337]
# validation_0_0005 = [0.5125888768747273 ,0.687676883825826 ,0.702198408706337 ,0.6716029492454878 ,0.7530816264712339 ,0.7571227355882485]
# test_0_0005 = 0.7132057627329113
# train_0_0001 = [0.38122782292992685, 0.5051147409753363, -0.056763746238680246, 0.18053078400231404, 0.46666829860951936, 0.5288038485166242]
# validation_0_0001 = [0.5258947495140377 ,0.1014539636585413 ,0.1885468649936487 ,0.2412200352247584 ,0.5435846184992953 ,0.589418906032774]
# test_0_0001 = 0.5028354727081477
# validation_neural = [0.8712111556178493 ,0.890942396975058 ,0.890003631203485 ,0.8936382137796928 ,0.8929555851953314 ,0.8939889133096588]
# test_pearson_neural = 0.8614684095989391
# test_spearmanr_neural = 0.8501692368159134

########################################################################
# roberta-base
########################################################################
# title = 'RoBERTa-base'
# train_0_01 = [0.12897953492257355, 0.13995027809211064, 0.2172969886556121, 0.27443609465297863, 0.21425754177736472, 0.3214843888408206]
# validation_0_01 = [0.3091229058444194 ,0.213405152590717 ,0.3446711263265548 ,0.3403480604509045 ,0.3517359220475269 ,0.3895673533321669]
# test_0_01 = 0.2958692493993311
# train_0_005 = [0.3729690275625902, 0.4276813722510339, 0.4057034504612365, 0.3297924194962099, 0.32782483788963057, 0.33955865487930964]
# validation_0_005 = [0.6131706409190719 ,0.3686805309799811 ,0.4912983868815777 ,0.468334485614418 ,0.4769504351520515 ,0.4843769252230509]
# test_0_005 = 0.40028473654198243
# train_0_001 = [0.5327007157229752, 0.21501783070560146, 0.12513833902357938, 0.34068942742892055, 0.26369593516938605, 0.29600550881305343]
# validation_0_001 = [0.4604447435778156 ,0.3695623646201358 ,0.2837604539387421 ,0.4250773880102248 ,0.4283951815049064 ,0.3965637903752871]
# test_0_001 = 0.280975857428627
# train_0_0005 = [0.44233480412433335, 0.2869117658313906, 0.3828803723748807, 0.3620734185538003, 0.4147705959533361, 0.47801696640506447]
# validation_0_0005 = [0.4841363370330772 ,0.4990394053217371 ,0.4745848913891444 ,0.515645989331782 ,0.5087423568935748 ,0.5155348362299654]
# test_0_0005 = 0.4697552936037066
# train_0_0001 = [0.3078322470437032, 0.2526726574545404, 0.3449640437278883, 0.3673752777973239, 0.5450552696666799, 0.48309038706053054]
# validation_0_0001 = [0.2053587644516889 ,0.437902110196973 ,0.3756134402656606 ,0.4619419124811293 ,0.4838446780943798 ,0.4504791199109074]
# test_0_0001 = 0.3586795467342528
# validation_neural = [0.8910174646424545 ,0.895005004757302 ,0.9017796660101762 ,0.9019763084944608 ,0.9033954286896312 ,0.9044488422270548]
# test_pearson_neural = 0.8971085090689782
# test_spearmanr_neural = 0.8909583416920509

########################################################################
# sentence-transformers/nli-distilroberta-base-v2
########################################################################
# title = 'NLI-DistilRoBERTa-base-v2'
# train_0_01 = [0.8290864879124278, 0.8353707743268753, 0.8395399350534166, 0.8423213696175129, 0.853713160769086, 0.8559203849619771]
# validation_0_01 = [0.8598924599690069 ,0.8606947719105913 ,0.8607906296279892 ,0.8574569671850446 ,0.865107862144724 ,0.8658448039344342]
# test_0_01 = 0.8266299088716405
# train_0_005 = [0.8304337407991339, 0.8313192367797438, 0.8405294904552997, 0.8543161534119825, 0.8591214012984705, 0.8615178633369197]
# validation_0_005 = [0.8572025132727883 ,0.8461356995856399 ,0.8509670321012889 ,0.8574963070698446 ,0.8606734943158362 ,0.8620088955932829]
# test_0_005 = 0.831851310717841
# train_0_001 = [0.8204965674599016, 0.8314535291774998, 0.8442080385270145, 0.8502112538668869, 0.8572557281011274, 0.8596072880182574]
# validation_0_001 = [0.8530922613259393 ,0.8506573450548304 ,0.855549764801248 ,0.858578202834266 ,0.8607018395311271 ,0.8613571869584783]
# test_0_001 = 0.83085471663051
# train_0_0005 = [0.8270109770338161, 0.8433791790508774, 0.8463726478462925, 0.8550342754439839, 0.8622430330502365, 0.86403878199774]
# validation_0_0005 = [0.8558642294310749 ,0.8581723555790975 ,0.8569167479510235 ,0.8630228375741538 ,0.8657647807475368 ,0.8659513556093902]
# test_0_0005 = 0.838209791649207
# train_0_0001 = [0.8295993741012915, 0.8349999128302138, 0.8435203197895009, 0.8552627674506703, 0.8602256974199041, 0.8617117018985881]
# validation_0_0001 = [0.857475803496976 ,0.8550980580838267 ,0.8559550612154616 ,0.861738831349969 ,0.8637318576364098 ,0.8632143732935232]
# test_0_0001 = 0.8330653211910718
# validation_neural = [0.8631444454317626 ,0.8712400791253285 ,0.8776951511899633 ,0.8793487341801824 ,0.8808269362666071 ,0.881739678888572]
# test_pearson_neural = 0.8564687482536639
# test_spearmanr_neural = 0.8460545414585718

########################################################################
# sentence-transformers/stsb-roberta-base-v2
########################################################################
# title = 'STS-B-RoBERTa-base-v2'
# train_0_01 = [0.940746694322019, 0.9329798627515715, 0.9410194495755464, 0.9400415165614919, 0.9433563698984923, 0.946262576023218]
# validation_0_01 = [0.8694680857943303 ,0.8630114283136111 ,0.8637812366740456 ,0.8678573035439223 ,0.8676918301201187 ,0.8692930892322821]
# test_0_01 = 0.8415391883307727
# train_0_005 = [0.942974175205186, 0.9323563128679732, 0.9449020542886802, 0.9432369753832387, 0.9450319182917999, 0.94801994185609]
# validation_0_005 = [0.8756598053697932 ,0.8733675546775663 ,0.8729092180623309 ,0.8705615367268683 ,0.8718956479678375 ,0.8742521994161828]
# test_0_005 = 0.8446147097852997
# train_0_001 = [0.9456970367192026, 0.9364386385466761, 0.9382174919134805, 0.9427671223716021, 0.9444333533760719, 0.9465331544856331]
# validation_0_001 = [0.8738608557297424 ,0.8676424154952699 ,0.8692502404482635 ,0.8705123186054811 ,0.871394334454616 ,0.8727199510278011]
# test_0_001 = 0.843042344545786
# train_0_0005 = [0.9437228978061808, 0.9387279631408167, 0.9424260378650952, 0.9430027157314578, 0.947976880222853, 0.9490519123098814]
# validation_0_0005 = [0.8747169079219366 ,0.8773421944311838 ,0.8740282384075325 ,0.8759622077137156 ,0.8769340304484378 ,0.8776615167156084]
# test_0_0005 = 0.8497137965945415
# train_0_0001 = [0.9396081368014957, 0.9349359283234498, 0.9419498401466802, 0.9389207245328484, 0.9442764113754293, 0.9461574519482607]
# validation_0_0001 = [0.8717699539309168 ,0.8668377868261081 ,0.8677012020127074 ,0.8683259871413872 ,0.8703721878417484 ,0.8700914812423358]
# test_0_0001 = 0.8408114763162501
# validation_neural = [0.8952791045903457 ,0.8996136342681894 ,0.9029238732044826 ,0.8986295606362079 ,0.9025346754273422 ,0.9039546248045788]
# test_pearson_neural = 0.892941373330529
# test_spearmanr_neural = 0.889197823777548

########################################################################
# albert-base-v2
########################################################################
# title = 'AlBERT-base-v2'
# train_0_01 = [0.22738359469426084, 0.34238466740840867, 0.35994204516412737, 0.41351744857953493, 0.5486273821704934, 0.6041814705494728]
# validation_0_01 = [0.2670394457348268 ,0.2911163749716638 ,0.2295470859812746 ,0.2166365932877195 ,0.2061537700694185 ,0.2209178851491162]
# test_0_01 = 0.22195562362173463
# train_0_005 = [0.4450117536439552, 0.5907846966124146, 0.661581404602748, 0.7006927462608382, 0.7042776916598845, 0.7214421122288374]
# validation_0_005 = [0.6196750166123143 ,0.7421283840368395 ,0.7643147196325639 ,0.7668842576746929 ,0.7740846872290154 ,0.7740626178187673]
# test_0_005 = 0.6974394691690439
# train_0_001 = [0.5791962896902577, 0.7055593414734087, 0.733359712541668, 0.7538717206467185, 0.7683214041025459, 0.7787539685597681]
# validation_0_001 = [0.7734956725876547 ,0.791839678129611 ,0.7949168581891488 ,0.8051437460297515 ,0.8058629838972318 ,0.8068552151823849]
# test_0_001 = 0.7267491067566418
# train_0_0005 = [0.5492272363525346, 0.7167364788418769, 0.7447542218401989, 0.7587163822983374, 0.7703669269955685, 0.7800307160903073]
# validation_0_0005 = [0.7672943656093228 ,0.7962369320538826 ,0.8061416613091708 ,0.8096881655360105 ,0.8103933973394246 ,0.8122795214168596]
# test_0_0005 = 0.7303756343084354
# train_0_0001 = [0.5797958294958602, 0.6925289222524018, 0.7279261315296545, 0.7381508750876197, 0.7440247143395325, 0.748696337187928]
# validation_0_0001 = [0.7691602710192353 ,0.7897130919547455 ,0.7962678128581119 ,0.8017740195049791 ,0.8001223617815225 ,0.8027549917122074]
# test_0_0001 = 0.7186292446249954
# validation_neural = [0.8800951546234573 ,0.8998882518951844 ,0.8971704937942884 ,0.9010668470135476 ,0.9042376997327762 ,0.9044854561312934]
# test_pearson_neural = 0.8809675208590636
# test_spearmanr_neural = 0.8718448911990516

########################################################################
# albert-xlarge-v2
########################################################################
# title = 'AlBERT-xlarge-v2'
# train_0_01 = [0.2621935608759819, 0.49178596591852713, 0.5420620345835105, 0.5467852194498607, 0.5034144543593523, 0.536731606961439]
# validation_0_01 = [0.5943069426943066 ,0.5521139517951962 ,0.6295992666737434 ,0.590280361095432 ,0.566528153926222 ,0.6158167419127901]
# test_0_01 = 0.5198835951361003
# train_0_005 = [0.4094862643798465, 0.07418852579918707, 0.038601037259431155, 0.16878698845594492, 0.05982555424582029, 0.061701729209187206]
# validation_0_005 = [0.37590500641288 ,0.1670340478047954 ,0.1554563882827193 ,0.1454918047641169 ,0.1048925300454791 ,0.1072860512585278]
# test_0_005 = 0.08385905282367503
# train_0_001 = [0.01994124422325617, 0, 0, 0, 0, 0]
# validation_0_001 = [0.054394933331557795, 0, 0, 0, 0, 0]
# test_0_001 = 0
# train_0_0005 = [0.5176018229539321, 0.6294804947056417, 0.6728631864085053, 0.6944680551676573, 0.7180321190933998, 0.7301414609020146]
# validation_0_0005 = [0.6770367371171727 ,0.6399229169545636 ,0.6655752911711459 ,0.666252638911135 ,0.6945098153152306 ,0.6919732367095057]
# test_0_0005 = 0.6625599837236011
# train_0_0001 = [0.5239002195348825, 0.5996870897508136, 0.6569865727947221, 0.7034804732611174, 0.7097518126003786, 0.7173123345765441]
# validation_0_0001 = [0.6514606571437984 ,0.6997909898697746 ,0.6652605062564005 ,0.6951383017864182 ,0.6831889441783117 ,0.6956981646195463]
# test_0_0001 = 0.6427923653140514
# validation_neural = [0.8799110566318045 ,0.8972228977449969 ,0.909384009329547 ,0.9160913314926614 ,0.9153413639692932 ,0.9173948137831522]
# test_pearson_neural = 0.8972504527935602
# test_spearmanr_neural = 0.8896138199961061

########################################################################
# microsoft/deberta-base
########################################################################
# title = 'DeBERTa-base'
# train_0_01 = [0.30560017131884526, 0.3875608832082273, 0.4130736028002965, 0.4366776645897045, 0.48722039729632133, 0.5070445800769076]
# validation_0_01 = [0.3728385585343933 ,0.4904238751279528 ,0.5110085467904272 ,0.5677722229481665 ,0.5736831848555812 ,0.5880943349774018]
# test_0_01 = 0.48409141472201583
# train_0_005 = [0.5701702406996685, 0.5777264456692957, 0.553410898127481, 0.5264251337638429, 0.5578103580197659, 0.5715064502468155]
# validation_0_005 = [0.68360174816922 ,0.7200293693171641 ,0.6499136119014087 ,0.6161421763016555 ,0.6566015915833455 ,0.671200400945996]
# test_0_005 = 0.5872668067810977
# train_0_001 = [0.5143657873441524, 0.5069279960448502, 0.6390070762788178, 0.6431716767090774, 0.6619296678040822, 0.6636555286735317]
# validation_0_001 = [0.6808732100913923 ,0.7323937847921982 ,0.7454553044822491 ,0.7592833227071477 ,0.7695979688301919 ,0.7699426814962476]
# test_0_001 = 0.687763354197377
# train_0_0005 = [0.571945917019534, 0.6605819220888484, 0.6807420619186718, 0.6826164812997771, 0.6882684957134625, 0.6891185781303077]
# validation_0_0005 = [0.7438412695605571 ,0.7811494593121099 ,0.788456389317899 ,0.7870485876346162 ,0.7905836274407329 ,0.7899069775075773]
# test_0_0005 = 0.7016700312051531
# train_0_0001 = [0.5383549562278817, 0.6359476876744846, 0.6651095189879108, 0.6809488023969503, 0.6984567278081765, 0.7015114001299949]
# validation_0_0001 = [0.6734550483884099 ,0.7669775595485101 ,0.7708965782673016 ,0.7862219180249768 ,0.7945409636182048 ,0.7964961937986382]
# test_0_0001 = 0.7159522229078484
# validation_neural = [0.8974445615988402 ,0.9001835795008658 ,0.9060947543934816 ,0.907503436776028 ,0.9053982361198316 ,0.907505045401072]
# test_pearson_neural = 0.9033888779094621
# test_spearmanr_neural = 0.897751444649394

########################################################################
# microsoft/deberta-large
########################################################################
# title = 'DeBERTa-large'
# train_0_01 = [0.26944748587539874, 0.29895284111235826, 0.5107700981055525, 0.5634999705962463, 0.63763998637687, 0.6945989558053947]
# validation_0_01 = [0.3845127468380497 ,0.3502995383639888 ,0.4660694121153572 ,0.4601799582058825 ,0.4736801652411452 ,0.4942697706624531]
# test_0_01 = 0.46611046479864454
# train_0_005 = [0.5967402708566962, 0.6344431316626122, 0.6999799990411351, 0.7022118661687567, 0.7191448153883601, 0.7273040673226564]
# validation_0_005 = [0.7510956357768371 ,0.763209328089055 ,0.7887247631685871 ,0.7878185828279198 ,0.7965495888532803 ,0.798611894546962]
# test_0_005 = 0.7371391441323346
# train_0_001 = [0.599693045930752, 0.6380070792649174, 0.11354283678839884, 0.0016980944842717785, 0.0016980944842717785, 0.0016980944842717785]
# validation_0_001 = [0.6886158382104633 ,0.7607024377802591 ,0.0750167782303774, 0, 0 , 0]
# test_0_001 = 0
# train_0_0005 = [0.5313647431047075, 0.642611625002268, 0.6643501476205217, 0.6908866263385095, 0.701919808863362, 0.7122222053357374]
# validation_0_0005 = [0.6381118644225631 ,0.7613422547348511 ,0.7656535016952618 ,0.7779487913191624 ,0.7889192170394366 ,0.793537529650811]
# test_0_0005 = 0.7277924019740419
# train_0_0001 = [0.6321478864790461, 0.6094254805502665, 0.6257447157134076, 0.6931458898928038, -0.02117169946994342, 0.2533777239016507]
# validation_0_0001 = [0.6595384320228386 ,0.7359967748231442 ,0.7519053818872529 ,0.7965919732241671 ,0.1234942717313874 ,0.4048079205208713]
# test_0_0001 = 0.2562615551723
# validation_neural = [0.9082659089147996 ,0.9156535421203964 ,0.92128582440128 ,0.9195889825552076 ,0.9232056576457636 ,0.9213696973056468]
# test_pearson_neural = 0.9112602637223857
# test_spearmanr_neural = 0.9089838648335854



########################################################################
# Polski
########################################################################
# sdadas/polish-roberta-base-v1
########################################################################
title = 'Polish-RoBERTa-base-v1'
train_0_01 = [0.8158080493671671, 0.844967493201848, 0.865326383059762, 0.8776371983107119, 0.8863862120218668, 0.8878881790869735]
validation_0_01 = [0.8211480883111018 , 0.8327617880768565 , 0.8329235585394821 , 0.842437503508553 , 0.8461811347880638 , 0.8485884528907026]
test_0_01 = 0.846390349368503
train_0_005 = [0.8361231690591497, 0.8652587497327376, 0.8639572561012324, 0.887650558565013, 0.8927242842579748, 0.8937147812263814]
validation_0_005 = [0.8230312888844304 , 0.8235028515426337 , 0.8317402058457266 , 0.8494426807041522 , 0.8494487104371318 , 0.8512347079609092]
test_0_005 = 0.8590291990246907
train_0_001 = [0.8332095267512347, 0.8592513891252124, 0.8746954237910092, 0.8873417780916891, 0.8936180510268659, 0.8952199947855205]
validation_0_001 = [0.8032920312266649 , 0.8248642104002675 , 0.8260368174998415 , 0.8402320993437467 , 0.8404643965664378 , 0.8419428894392506]
test_0_001 = 0.858618086513126
train_0_0005 = [0.8409160162622308, 0.8692442258570126, 0.8830007214032213, 0.8895269426207016, 0.8982801576906159, 0.9005200800624453]
validation_0_0005 = [0.8282016676045102 , 0.840542302124255 , 0.8530398270114509 , 0.8588223644009424 , 0.8624048000367304 , 0.8646490689979601]
test_0_0005 = 0.8700386758948289
train_0_0001 = [0.8344245878660067, 0.8642301927882069, 0.8737049218755361, 0.890247134905547, 0.8965426659360642, 0.8977593363550599]
validation_0_0001 = [0.8280614939092302 , 0.8264063322063131 , 0.8426778247338864 , 0.8436903271725529 , 0.8524948752296664 , 0.8554941512231354]
test_0_0001 = 0.861685144776211
validation_neural = [0.8877517680205714 , 0.9136734610591588 , 0.9312060945283246 , 0.94534812937224 , 0.9507882858536686 , 0.9552181922480496]
test_pearson_neural = 0.9358199149054504
test_spearmanr_neural = 0.9284641669450696

########################################################################
# sdadas/polish-roberta-large-v2
########################################################################
# title = 'Polish-RoBERTa-large-v2'
# train_0_01 = [0.8532978656344321, 0.8525987103744876, 0.8643169769609156, 0.8904220287119385, 0.9008085325485959, 0.9028562149369257]
# validation_0_01 = [0.8260892315989722 , 0.8434311307714136 , 0.838330117442596 , 0.8557263780598384 , 0.8615480735207038 , 0.8640572636101042]
# test_0_01 = 0.8576121512633741
# train_0_005 = [0.8711275573300256, 0.8610734788181699, 0.8901905357330553, 0.9046017127570484, 0.9078764392294378, 0.9125084680467492]
# validation_0_005 = [0.8268946701916775 , 0.8160609644839337 , 0.8656174863460866 , 0.8582291278313285 , 0.8615948919337812 , 0.8656952394009293]
# test_0_005 = 0.882877057378876
# train_0_001 = [0.8544398600895527, 0.8557802948933717, 0.8801468809435922, 0.8946394800333353, 0.9002889951235747, 0.9045169933260366]
# validation_0_001 = [0.8305896295604245 , 0.8433568383609851 , 0.8594321534087551 , 0.8570454114766345 , 0.866626023707424 , 0.8690087653692217]
# test_0_001 = 0.8513148112169255
# train_0_0005 = [0.8664894260489681, 0.8833672186267902, 0.8839857873981303, 0.9072168405788343, 0.9099705859461815, 0.9123409178770101]
# validation_0_0005 = [0.8332978712241417 , 0.8450726141306453 , 0.8760239256441315 , 0.8826407716124433 , 0.8780190047455285 , 0.8817380361222021]
# test_0_0005 = 0.8577178002460071
# train_0_0001 = [0.8761292345858592, 0.8836492948524591, 0.894292757628279, 0.910243457272706, 0.9121099425068457, 0.9163994829322479]
# validation_0_0001 = [0.8573817157277259 , 0.8595057419593027 , 0.8753676936151776 , 0.8801168173534846 , 0.8789950472423649 , 0.8823909013547258]
# test_0_0001 = 0.8771188394660324
# validation_neural = [0.920857360009515 , 0.9491416173933144 , 0.962422855064892 , 0.9688907290894124 , 0.9737963509152938 , 0.9751142112322884]
# test_pearson_neural = 0.95865681703791
# test_spearmanr_neural = 0.9519297309419733

########################################################################
# sdadas/polish-distilroberta
########################################################################
# title = 'Polish-DistilRoBERTa'
# train_0_01 = [0.6731640322430549, 0.7083782186266847, 0.6725490289131677, 0.6876034529211771, 0.6691156261620288, 0.6833384610225106]
# validation_0_01 = [0.6595374136095096 , 0.6425168030107004 , 0.6328801400713955 , 0.6487686507067763 , 0.6320761091984519 , 0.6422281890296011]
# test_0_01 = 0.6418642556540657
# train_0_005 = [0.6842893341448665, 0.6172281337073127, 0.5933120047264666, 0.596376554979308, 0.5971344928742928, 0.5958893108626162]
# validation_0_005 = [0.6225051281170325 , 0.5777008452817161 , 0.5513422429530616 , 0.5438873200769127 , 0.5523705587770522 , 0.5470470905328779]
# test_0_005 = 0.5685431830067282
# train_0_001 = [0.6459883415156219, 0.6401235256144067, 0.6114019716759889, 0.5968950787236484, 0.6059674114562211, 0.6117413929994979]
# validation_0_001 = [0.6187844778626329 , 0.6132521626607993 , 0.5632901705832591 , 0.5301778950319631 , 0.567372956746379 , 0.5670363005653473]
# test_0_001 = 0.5985073264330469
# train_0_0005 = [0.7484698741223719, 0.6620250585735504, 0.6127346584632963, 0.6157870868726815, 0.6113106540359825, 0.6099820725448236]
# validation_0_0005 = [0.6655531984325315 , 0.6131833368954458 , 0.5665989454973553 , 0.5637960555796526 , 0.5706449172668331 , 0.5724077837843964]
# test_0_0005 = 0.5994849038599025
# train_0_0001 = [0.6597727799400507, 0.5784277810848629, 0.6099416493972959, 0.6144058785335108, 0.615758496165094, 0.619628606974079]
# validation_0_0001 = [0.6622445994834054 , 0.5998713053540788 , 0.5677155957363983 , 0.5589788880408051 , 0.5820185798730784 , 0.5755912599086955]
# test_0_0001 = 0.6059278893508938
# validation_neural = [0.6471268388796 , 0.8843960339214214 , 0.9108551239790716 , 0.9268554310255344 , 0.9338644677194804 , 0.9368773868636407]
# test_pearson_neural = 0.9242912372070636
# test_spearmanr_neural = 0.9207260907014033

########################################################################
# allegro/herbert-base-cased
########################################################################
# title = 'HerBERT-base-cased'
# train_0_01 = [0.6024739239360725, 0.22204189880424405, -0.02238206539418182, -0.07121793074084078, -0.03046822082318447, -0.024736852550447688]
# validation_0_01 = [0.3081983315031106 ,0.0058547417851888 ,0.0113310707624738 ,-0.0046262716514736 ,-0.0016130239769502 ,0.0059849909883011]
# test_0_01 = -0.015578822583837101
# train_0_005 = [0.6527432833863521, 0.664213457723636, 0.6925941336632107, 0.7168009567567737, 0.7300753135365959, 0.7363910829962576]
# validation_0_005 = [0.6695795115712074 , 0.655169773325984 , 0.6681701260077005 , 0.6674624183594574 , 0.6732715421354072 , 0.6738967534606531]
# test_0_005 = 0.6655556045603005
# train_0_001 = [0.7773462885835914, 0.8273218324499694, 0.8531659627759388, 0.8636586066303716, 0.871744136273452, 0.8735705518752724]
# validation_0_001 = [0.7745439067193602 , 0.7976559084233974 , 0.8040591563798537 , 0.8216411186956721 , 0.8270746588942203 , 0.8280165759178311]
# test_0_001 = 0.807999280907449
# train_0_0005 = [0.778233634883998, 0.8164288332619406, 0.8412847698609752, 0.8493902630537477, 0.8579423076382348, 0.8608809220657165]
# validation_0_0005 = [0.7721369522776598 , 0.78350102686199 , 0.79824986058389 , 0.8098457057706664 , 0.8128563021772197 , 0.8139502177421097]
# test_0_0005 = 0.7961715139268567
# train_0_0001 = [0.7496082250051584, 0.7822112966422934, 0.8370579025273738, 0.85449386523105, 0.8612210253401085, 0.8637341308345654]
# validation_0_0001 = [0.7405982814245652 , 0.7969900687070693 , 0.8148776352521742 , 0.817321988115546 , 0.8206343761770309 , 0.8272292382550628]
# test_0_0001 = 0.8110005829126016
# validation_neural = [0.833837098278895 , 0.8946780659855185 , 0.9134802632528513 , 0.9212858464430076 , 0.9321678369385806 , 0.9351774714032968]
# test_pearson_neural = 0.9217878994117525
# test_spearmanr_neural = 0.9207588364804431



########################################################################
# Plotting part
########################################################################


x = range(1, len(train_0_01)+1)
# scalling from <0,1> to range <0, 100>
train_0_01 = [abs(i*100) for i in train_0_01]
validation_0_01 = [abs(i*100) for i in validation_0_01]
train_0_005 = [abs(i*100) for i in train_0_005]
validation_0_005 = [abs(i*100) for i in validation_0_005]
train_0_001 = [abs(i*100) for i in train_0_001]
validation_0_001 = [abs(i*100) for i in validation_0_001]
train_0_0005 = [abs(i*100) for i in train_0_0005]
validation_0_0005 = [abs(i*100) for i in validation_0_0005]
train_0_0001 = [abs(i*100) for i in train_0_0001]
validation_0_0001 = [abs(i*100) for i in validation_0_0001]
validation_neural = [abs(i*100) for i in validation_neural]


xsize = 8
ysize = 5
plt.figure( figsize=(xsize, ysize) )

plt.plot( x, train_0_01, '-o', label=r'$\lambda$ = 0.01, trening', lw=1.1, ms=4 , c='#FF9A49' )
plt.plot( x, validation_0_01, '-D', label=r'$\lambda$ = 0.01, walidacja', lw=0.8, ms=3.2, c='#FFCC66' )
plt.plot( x, train_0_005, '-o', label=r'$\lambda$ = 0.005, trening', lw=1.1, ms=4 , c='#D21E05' )
plt.plot( x, validation_0_005, '-D', label=r'$\lambda$ = 0.005, walidacja', lw=0.8, ms=3.2, c='#F39E9E' )
plt.plot( x, train_0_001, '-o', label=r'$\lambda$ = 0.001, trening', lw=1.1, ms=4 , c='#613714' )
plt.plot( x, validation_0_001, '-D', label=r'$\lambda$ = 0.001, walidacja', lw=0.8, ms=3.2, c='#A78B67' )
plt.plot( x, train_0_0005, '-o', label=r'$\lambda$ = 0.0005, trening', lw=1.1, ms=4 , c='#1F44A3' )
plt.plot( x, validation_0_0005, '-D', label=r'$\lambda$ = 0.0005, walidacja', lw=0.8, ms=3.2, c='#79C1E8' )
plt.plot( x, train_0_0001, '-o', label=r'$\lambda$ = 0.0001, trening', lw=1.1, ms=4 , c='#188977' )
plt.plot( x, validation_0_0001, '-D', label=r'$\lambda$ = 0.0001, walidacja', lw=0.8, ms=3.2, c='#6FC486' )
plt.plot( x, validation_neural, '--D', label=r'Model solowo, wal.', lw=1.1, ms=4, c='k' ) # spearmanr 

ax = plt.gca()
ax.xaxis.set_minor_locator( MultipleLocator(1) )
ax.yaxis.set_minor_locator( MultipleLocator(.5) )

if ax.get_ylim()[0] < 0: #bottom
    ax.set_ylim(bottom=0)

plt.xlabel( 'Liczba epok', labelpad=10 )
plt.xticks( x )
plt.ylabel( 'Dokładność (accuracy), %', labelpad=20 )
plt.title( title, pad=10 )
plt.legend( loc='best' )
    
plt.show()



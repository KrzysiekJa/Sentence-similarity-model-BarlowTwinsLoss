import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator


fsize = 15
tsize = 15
tdir = 'in'
major = 5.0
minor = 3.0
style = 'default'
lwidth = 0.5
lhandle = 3.0

plt.style.use(style)
plt.rcParams.update(plt.rcParamsDefault)
#plt.rcParams['text.usetex'] = True
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = fsize
plt.rcParams['legend.fontsize'] = tsize
plt.rcParams['xtick.direction'] = tdir
plt.rcParams['ytick.direction'] = tdir
plt.rcParams['xtick.major.size'] = major
plt.rcParams['xtick.minor.size'] = minor
plt.rcParams['ytick.major.size'] = major
plt.rcParams['ytick.minor.size'] = minor
plt.rcParams['axes.linewidth'] = lwidth
plt.rcParams['legend.handlelength'] = lhandle


########################################################################
# English
########################################################################
# bert-base-uncased
########################################################################
# title = 'BERT-base-uncased'
# train_0_01 = [0.6670863812934525, 0.7279276521899394, 0.7521322658828504, 0.7694349361328888, 0.7784602350525042, 0.7811061279783081]
# validation_0_01 = [0.7724997537508943 ,0.7933758909335401 ,0.8040932880094456 ,0.8074276337268831 ,0.810817984821049 ,0.812024127834295]
# test_0_01 = 0.7594756489046671
# train_0_005 = [0.6782729681792927, 0.7276760038208191, 0.7520256042465717, 0.7665433580224204, 0.7767568474645611, 0.7793812999842847]
# validation_0_005 = [0.7841141265347802 ,0.8028004472595911 ,0.8051851464141943 ,0.8058985668353588 ,0.8082816519386372 ,0.8089667130808331]
# test_0_005 = 0.7499771084578993
# train_0_001 = [0.6295178463759806, 0.7164170105010886, 0.7439169495333177, 0.7585925254980911, 0.7682028384124757, 0.7706709560269178]
# validation_0_001 = [0.7647324494582103 ,0.7864160185020919 ,0.7976775166779387 ,0.7970589237243507 ,0.8015285015419501 ,0.8013391455393872]
# test_0_001 = 0.7530739129462626
# train_0_0005 = [0.6585387741469894, 0.7344295089448608, 0.7481653150302372, 0.763060853248611, 0.7718778580978425, 0.7754874374917793]
# validation_0_0005 = [0.7796803385808362 ,0.7988387363405186 ,0.800053948922922 ,0.8027205308343331 ,0.8040517310412648 ,0.8049082725576401]
# test_0_0005 = 0.7565474507901706
# train_0_0001 = [0.6662209552534816, 0.7269999250196131, 0.7476775707469635, 0.7628557633725177, 0.770138746315303, 0.774274483169599]
# validation_0_0001 = [0.7734572437355118 ,0.795097514897452 ,0.8024075306183283 ,0.8016611333060786 ,0.8048844540358647 ,0.8055636182056307]
# test_0_0001 = 0.7468513979945739

########################################################################
# bert-large-uncased
########################################################################
# title = 'BERT-large-uncased'
# train_0_01 = [0.3999860858905723, 0.1597188637223978, -0.02102328024764908, 0.047740869796194685, 0.11975678271474609, 0.1915015142197596]
# validation_0_01 = [0.3217473353165915 ,0.3203192716331857 ,0.2547408025354958 ,0.1920777697805262 ,0.2303793350388756 ,0.2765513041090314]
# test_0_01 = 0.237601446684278
# train_0_005 = [0.422186923070396, 0.5116502781258778, 0.3963250654024163, 0.48018139104775054, 0.47947527991243155, 0.48398645888372627]
# validation_0_005 = [0.609776384183033 ,0.5977247204983323 ,0.5215804031916045 ,0.5986427387525192 ,0.5927938242110199 ,0.6004907455437857]
# test_0_005 = 0.47373802139437254
# train_0_001 = [0.6090953952094409, 0.596145987199234, 0.6999225492950739, 0.7104836403741716, 0.7328757361948954, 0.7356189195761665]
# validation_0_001 = [0.4353897118861285 ,0.6294615002343985 ,0.665083618825158 ,0.7335370720580614 ,0.7419104728544282 ,0.7412318300373352]
# test_0_001 = 0.7019013384896126
# train_0_0005 = [0.41395359262518416, 0.6007667579368258, 0.6422142497194905, 0.6957332116628696, 0.7175762612141069, 0.7321280823227337]
# validation_0_0005 = [0.5125888768747273 ,0.687676883825826 ,0.702198408706337 ,0.6716029492454878 ,0.7530816264712339 ,0.7571227355882485]
# test_0_0005 = 0.7132057627329113
# train_0_0001 = [0.38122782292992685, 0.5051147409753363, -0.056763746238680246, 0.18053078400231404, 0.46666829860951936, 0.5288038485166242]
# validation_0_0001 = [0.5258947495140377 ,0.1014539636585413 ,0.1885468649936487 ,0.2412200352247584 ,0.5435846184992953 ,0.589418906032774]
# test_0_0001 = 0.5028354727081477

########################################################################
# roberta-base
########################################################################
# title = 'RoBERTa-base'
# train_0_01 = [0.12897953492257355, 0.13995027809211064, 0.2172969886556121, 0.27443609465297863, 0.21425754177736472, 0.3214843888408206]
# validation_0_01 = [0.3091229058444194 ,0.213405152590717 ,0.3446711263265548 ,0.3403480604509045 ,0.3517359220475269 ,0.3895673533321669]
# test_0_01 = 0.2958692493993311
# train_0_005 = [0.3729690275625902, 0.4276813722510339, 0.4057034504612365, 0.3297924194962099, 0.32782483788963057, 0.33955865487930964]
# validation_0_005 = [0.6131706409190719 ,0.3686805309799811 ,0.4912983868815777 ,0.468334485614418 ,0.4769504351520515 ,0.4843769252230509]
# test_0_005 = 0.40028473654198243
# train_0_001 = [0.5327007157229752, 0.21501783070560146, 0.12513833902357938, 0.34068942742892055, 0.26369593516938605, 0.29600550881305343]
# validation_0_001 = [0.4604447435778156 ,0.3695623646201358 ,0.2837604539387421 ,0.4250773880102248 ,0.4283951815049064 ,0.3965637903752871]
# test_0_001 = 0.280975857428627
# train_0_0005 = [0.44233480412433335, 0.2869117658313906, 0.3828803723748807, 0.3620734185538003, 0.4147705959533361, 0.47801696640506447]
# validation_0_0005 = [0.4841363370330772 ,0.4990394053217371 ,0.4745848913891444 ,0.515645989331782 ,0.5087423568935748 ,0.5155348362299654]
# test_0_0005 = 0.4697552936037066
# train_0_0001 = [0.3078322470437032, 0.2526726574545404, 0.3449640437278883, 0.3673752777973239, 0.5450552696666799, 0.48309038706053054]
# validation_0_0001 = [0.2053587644516889 ,0.437902110196973 ,0.3756134402656606 ,0.4619419124811293 ,0.4838446780943798 ,0.4504791199109074]
# test_0_0001 = 0.3586795467342528

########################################################################
# nli-distilroberta-base-v2
########################################################################
# title = 'NLI-DistilRoBERTa-base-v2'
# train_0_01 = [0.8290864879124278, 0.8353707743268753, 0.8395399350534166, 0.8423213696175129, 0.853713160769086, 0.8559203849619771]
# validation_0_01 = [0.8598924599690069 ,0.8606947719105913 ,0.8607906296279892 ,0.8574569671850446 ,0.865107862144724 ,0.8658448039344342]
# test_0_01 = 0.8266299088716405
# train_0_005 = [0.8304337407991339, 0.8313192367797438, 0.8405294904552997, 0.8543161534119825, 0.8591214012984705, 0.8615178633369197]
# validation_0_005 = [0.8572025132727883 ,0.8461356995856399 ,0.8509670321012889 ,0.8574963070698446 ,0.8606734943158362 ,0.8620088955932829]
# test_0_005 = 0.831851310717841
# train_0_001 = [0.8204965674599016, 0.8314535291774998, 0.8442080385270145, 0.8502112538668869, 0.8572557281011274, 0.8596072880182574]
# validation_0_001 = [0.8530922613259393 ,0.8506573450548304 ,0.855549764801248 ,0.858578202834266 ,0.8607018395311271 ,0.8613571869584783]
# test_0_001 = 0.83085471663051
# train_0_0005 = [0.8270109770338161, 0.8433791790508774, 0.8463726478462925, 0.8550342754439839, 0.8622430330502365, 0.86403878199774]
# validation_0_0005 = [0.8558642294310749 ,0.8581723555790975 ,0.8569167479510235 ,0.8630228375741538 ,0.8657647807475368 ,0.8659513556093902]
# test_0_0005 = 0.838209791649207
# train_0_0001 = [0.8295993741012915, 0.8349999128302138, 0.8435203197895009, 0.8552627674506703, 0.8602256974199041, 0.8617117018985881]
# validation_0_0001 = [0.857475803496976 ,0.8550980580838267 ,0.8559550612154616 ,0.861738831349969 ,0.8637318576364098 ,0.8632143732935232]
# test_0_0001 = 0.8330653211910718

########################################################################
# albert-base-v2
########################################################################
# title = 'AlBERT-base-v2'
# train_0_01 = [0.22738359469426084, 0.34238466740840867, 0.35994204516412737, 0.41351744857953493, 0.5486273821704934, 0.6041814705494728]
# validation_0_01 = [0.2670394457348268 ,0.2911163749716638 ,0.2295470859812746 ,0.2166365932877195 ,0.2061537700694185 ,0.2209178851491162]
# test_0_01 = 0.22195562362173463
# train_0_005 = [0.4450117536439552, 0.5907846966124146, 0.661581404602748, 0.7006927462608382, 0.7042776916598845, 0.7214421122288374]
# validation_0_005 = [0.6196750166123143 ,0.7421283840368395 ,0.7643147196325639 ,0.7668842576746929 ,0.7740846872290154 ,0.7740626178187673]
# test_0_005 = 0.6974394691690439
# train_0_001 = [0.5791962896902577, 0.7055593414734087, 0.733359712541668, 0.7538717206467185, 0.7683214041025459, 0.7787539685597681]
# validation_0_001 = [0.7734956725876547 ,0.791839678129611 ,0.7949168581891488 ,0.8051437460297515 ,0.8058629838972318 ,0.8068552151823849]
# test_0_001 = 0.7267491067566418
# train_0_0005 = [0.5492272363525346, 0.7167364788418769, 0.7447542218401989, 0.7587163822983374, 0.7703669269955685, 0.7800307160903073]
# validation_0_0005 = [0.7672943656093228 ,0.7962369320538826 ,0.8061416613091708 ,0.8096881655360105 ,0.8103933973394246 ,0.8122795214168596]
# test_0_0005 = 0.7303756343084354
# train_0_0001 = [0.5797958294958602, 0.6925289222524018, 0.7279261315296545, 0.7381508750876197, 0.7440247143395325, 0.748696337187928]
# validation_0_0001 = [0.7691602710192353 ,0.7897130919547455 ,0.7962678128581119 ,0.8017740195049791 ,0.8001223617815225 ,0.8027549917122074]
# test_0_0001 = 0.7186292446249954

########################################################################
# albert-xlarge-v2
########################################################################
title = 'AlBERT-xlarge-v2'
train_0_01 = [0.2621935608759819, 0.49178596591852713, 0.5420620345835105, 0.5467852194498607, 0.5034144543593523, 0.536731606961439]
validation_0_01 = [0.5943069426943066 ,0.5521139517951962 ,0.6295992666737434 ,0.590280361095432 ,0.566528153926222 ,0.6158167419127901]
test_0_01 = 0.5198835951361003
train_0_005 = [0.4094862643798465, 0.07418852579918707, 0.038601037259431155, 0.16878698845594492, 0.05982555424582029, 0.061701729209187206]
validation_0_005 = [0.37590500641288 ,0.1670340478047954 ,0.1554563882827193 ,0.1454918047641169 ,0.1048925300454791 ,0.1072860512585278]
test_0_005 = 0.08385905282367503
train_0_001 = [0.01994124422325617, 0, 0, 0, 0, 0]
validation_0_001 = [0.054394933331557795, 0, 0, 0, 0, 0]
test_0_001 = 0
train_0_0005 = [0.5176018229539321, 0.6294804947056417, 0.6728631864085053, 0.6944680551676573, 0.7180321190933998, 0.7301414609020146]
validation_0_0005 = [0.6770367371171727 ,0.6399229169545636 ,0.6655752911711459 ,0.666252638911135 ,0.6945098153152306 ,0.6919732367095057]
test_0_0005 = 0.6625599837236011
train_0_0001 = [0.5239002195348825, 0.5996870897508136, 0.6569865727947221, 0.7034804732611174, 0.7097518126003786, 0.7173123345765441]
validation_0_0001 = [0.6514606571437984 ,0.6997909898697746 ,0.6652605062564005 ,0.6951383017864182 ,0.6831889441783117 ,0.6956981646195463]
test_0_0001 = 0.6427923653140514

########################################################################
# microsoft/deberta-base
########################################################################
# title = 'DeBERTa-base'
# train_0_01 = [0.30560017131884526, 0.3875608832082273, 0.4130736028002965, 0.4366776645897045, 0.48722039729632133, 0.5070445800769076]
# validation_0_01 = [0.3728385585343933 ,0.4904238751279528 ,0.5110085467904272 ,0.5677722229481665 ,0.5736831848555812 ,0.5880943349774018]
# test_0_01 = 0.48409141472201583
# train_0_005 = [0.5701702406996685, 0.5777264456692957, 0.553410898127481, 0.5264251337638429, 0.5578103580197659, 0.5715064502468155]
# validation_0_005 = [0.68360174816922 ,0.7200293693171641 ,0.6499136119014087 ,0.6161421763016555 ,0.6566015915833455 ,0.671200400945996]
# test_0_005 = 0.5872668067810977
# train_0_001 = [0.5143657873441524, 0.5069279960448502, 0.6390070762788178, 0.6431716767090774, 0.6619296678040822, 0.6636555286735317]
# validation_0_001 = [0.6808732100913923 ,0.7323937847921982 ,0.7454553044822491 ,0.7592833227071477 ,0.7695979688301919 ,0.7699426814962476]
# test_0_001 = 0.687763354197377
# train_0_0005 = [0.571945917019534, 0.6605819220888484, 0.6807420619186718, 0.6826164812997771, 0.6882684957134625, 0.6891185781303077]
# validation_0_0005 = [0.7438412695605571 ,0.7811494593121099 ,0.788456389317899 ,0.7870485876346162 ,0.7905836274407329 ,0.7899069775075773]
# test_0_0005 = 0.7016700312051531
# train_0_0001 = [0.5383549562278817, 0.6359476876744846, 0.6651095189879108, 0.6809488023969503, 0.6984567278081765, 0.7015114001299949]
# validation_0_0001 = [0.6734550483884099 ,0.7669775595485101 ,0.7708965782673016 ,0.7862219180249768 ,0.7945409636182048 ,0.7964961937986382]
# test_0_0001 = 0.7159522229078484

########################################################################
# microsoft/deberta-large
########################################################################
# title = 'DeBERTa-large'
# train_0_01 = [0.26944748587539874, 0.29895284111235826, 0.5107700981055525, 0.5634999705962463, 0.63763998637687, 0.6945989558053947]
# validation_0_01 = [0.3845127468380497 ,0.3502995383639888 ,0.4660694121153572 ,0.4601799582058825 ,0.4736801652411452 ,0.4942697706624531]
# test_0_01 = 0.46611046479864454
# train_0_005 = [0.5967402708566962, 0.6344431316626122, 0.6999799990411351, 0.7022118661687567, 0.7191448153883601, 0.7273040673226564]
# validation_0_005 = [0.7510956357768371 ,0.763209328089055 ,0.7887247631685871 ,0.7878185828279198 ,0.7965495888532803 ,0.798611894546962]
# test_0_005 = 0.7371391441323346
# train_0_001 = [0.599693045930752, 0.6380070792649174, 0.11354283678839884, 0.0016980944842717785, 0.0016980944842717785, 0.0016980944842717785]
# validation_0_001 = [0.6886158382104633 ,0.7607024377802591 ,0.0750167782303774, 0, 0 , 0]
# test_0_001 = 0
# train_0_0005 = [0.5313647431047075, 0.642611625002268, 0.6643501476205217, 0.6908866263385095, 0.701919808863362, 0.7122222053357374]
# validation_0_0005 = [0.6381118644225631 ,0.7613422547348511 ,0.7656535016952618 ,0.7779487913191624 ,0.7889192170394366 ,0.793537529650811]
# test_0_0005 = 0.7277924019740419
# train_0_0001 = [0.6321478864790461, 0.6094254805502665, 0.6257447157134076, 0.6931458898928038, -0.02117169946994342, 0.2533777239016507]
# validation_0_0001 = [0.6595384320228386 ,0.7359967748231442 ,0.7519053818872529 ,0.7965919732241671 ,0.1234942717313874 ,0.4048079205208713]
# test_0_0001 = 0.2562615551723

########################################################################
# 
########################################################################



########################################################################
# Polski
########################################################################
# 
########################################################################
# title = ''
# train_0_01 = []
# validation_0_01 = []
# test_0_01 = 0


########################################################################
# Plotting part
########################################################################


x = range(1, len(train_0_01)+1)
# scalling from <0,1> to range <0, 100>
train_0_01 = [i*100 for i in train_0_01]
validation_0_01 = [i*100 for i in validation_0_01]
train_0_005 = [i*100 for i in train_0_005]
validation_0_005 = [i*100 for i in validation_0_005]
train_0_001 = [i*100 for i in train_0_001]
validation_0_001 = [i*100 for i in validation_0_001]
train_0_0005 = [i*100 for i in train_0_0005]
validation_0_0005 = [i*100 for i in validation_0_0005]
train_0_0001 = [i*100 for i in train_0_0001]
validation_0_0001 = [i*100 for i in validation_0_0001]


xsize = 8
ysize = 5
plt.figure( figsize=(xsize, ysize) )

plt.plot( x, train_0_01, '-o', label=r'$\lambda$ = 0.01, trening', lw=1.1, ms=4 , c='#FF9A49' )
plt.plot( x, validation_0_01, '-h', label=r'$\lambda$ = 0.01, walidacja', lw=0.8, ms=3.5 , c='#FFCC66' )
plt.plot( x, train_0_005, '-o', label=r'$\lambda$ = 0.005, trening', lw=1.1, ms=4 , c='#D21E05' )
plt.plot( x, validation_0_005, '-h', label=r'$\lambda$ = 0.005, walidacja', lw=0.8, ms=3.5 , c='#F39E9E' )
plt.plot( x, train_0_001, '-o', label=r'$\lambda$ = 0.001, trening', lw=1.1, ms=4 , c='#613714' )
plt.plot( x, validation_0_001, '-h', label=r'$\lambda$ = 0.001, walidacja', lw=0.8, ms=3.5 , c='#A78B67' )
plt.plot( x, train_0_0005, '-o', label=r'$\lambda$ = 0.0005, trening', lw=1.1, ms=4 , c='#1F44A3' )
plt.plot( x, validation_0_0005, '-h', label=r'$\lambda$ = 0.0005, walidacja', lw=0.8, ms=3.5 , c='#79C1E8' )
plt.plot( x, train_0_0001, '-o', label=r'$\lambda$ = 0.0001, trening', lw=1.1, ms=4 , c='#188977' )
plt.plot( x, validation_0_0001, '-h', label=r'$\lambda$ = 0.0001, walidacja', lw=0.8, ms=3.5 , c='#6FC486' )

ax = plt.gca()
ax.xaxis.set_minor_locator( MultipleLocator(1) )
ax.yaxis.set_minor_locator( MultipleLocator(.5) )

if ax.get_ylim()[0] < 0: #bottom
    ax.set_ylim(bottom=0)

plt.xlabel( 'Liczba epok', labelpad=10 )
plt.xticks( x )
plt.ylabel( 'Dokładność (accuracy), %', labelpad=20 )
plt.title( title, pad=10 )
plt.legend( loc='best' )
    
plt.show()



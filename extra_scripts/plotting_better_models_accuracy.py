import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator


fsize = 15
tsize = 13
tdir = 'in'
major = 5.0
minor = 3.0
style = 'default'
lwidth = 0.5
lhandle = 3.0

plt.style.use(style)
plt.rcParams.update(plt.rcParamsDefault)
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = fsize
plt.rcParams['legend.fontsize'] = tsize
plt.rcParams['xtick.direction'] = tdir
plt.rcParams['ytick.direction'] = tdir
plt.rcParams['xtick.major.size'] = major
plt.rcParams['xtick.minor.size'] = minor
plt.rcParams['ytick.major.size'] = major
plt.rcParams['ytick.minor.size'] = minor
plt.rcParams['axes.linewidth'] = lwidth
plt.rcParams['legend.handlelength'] = lhandle


xsize = 8
ysize = 5
plt.figure( figsize=(xsize, ysize) )



########################################################################
# English
########################################################################
# sentence-transformers/nli-distilroberta-base-v2
nli_distilroberta_train_0_0005 = [0.8298141913861264, 0.8399417394508042, 0.8406221664231945, 0.8574063408428723, 0.8676498851158406, 0.8736073426850003, 0.8775488088511079, 0.8842445309541175, 0.8881314422308, 0.8963682450271784, 0.8990843350960087, 0.9006343803139687, 0.9036182205118175, 0.9045361360171177, 0.9077436848713755, 0.9073619087788856, 0.9100792148368497, 0.9108209916917569, 0.9114362029227393, 0.911913093329857]
nli_distilroberta_validation_0_0005 = [0.8630264977190284, 0.8622313405866521, 0.8628900944773175, 0.8653706274506164, 0.8681753268950662, 0.8688366600916728, 0.8647077533276795, 0.8660400833087626, 0.8680283855125216, 0.8680410189731973, 0.8670980813707998, 0.8673866377386837, 0.8679308480786619, 0.8672202568409444, 0.8683370227361915, 0.8685819819799737, 0.8676736076876145, 0.8671760237143332, 0.8672852177398671, 0.8673520424003793]
nli_distilroberta_test_0_0005 = 0.8420411781903934
nli_distilroberta_train_0_0001 = [0.8330648522650155, 0.8326201509937456, 0.8424953229781073, 0.8450184347951328, 0.8581421164282136, 0.8628685527476457, 0.8734892594143476, 0.8759678394594422, 0.8880668452734873, 0.8913288807673773, 0.8946087669547083, 0.8976553571521563, 0.901261544409886, 0.9029753680354388, 0.904679730375333, 0.9062135647486559, 0.9069406017897196, 0.9086937774237174, 0.9086996628612741, 0.9089558678520622]
nli_distilroberta_validation_0_0001 = [0.8581237914886434, 0.8538909006671245, 0.8568532799356933, 0.8599002695829373, 0.8590198391339336, 0.858528507070433, 0.8637986833053027, 0.8630909550588848, 0.8645105663588584, 0.8632689889895772, 0.8610820266262383, 0.8624603745525401, 0.864288137379735, 0.8653895064672037, 0.8638096299320742, 0.8638205765588456, 0.863359723927639, 0.8636763647155512, 0.8633720620144455, 0.8634945798925907]
nli_distilroberta_test_0_0001 = 0.8362310006212216
# sentence-transformers/stsb-roberta-base-v2
stsb_roberta_train_0_005 = [0.9618970581481523, 0.9410978296909103, 0.936403244512206, 0.9395683229287923, 0.9389095879313962, 0.9443815952153111, 0.9466640319829797, 0.9505392582814223, 0.95269199168866, 0.9544219645996574, 0.9562385803082584, 0.9585223148569219, 0.9591902924730443, 0.9600261515060714, 0.9621154821856647, 0.9629867086422376, 0.9632640255806425, 0.963604848036753, 0.9641178750452877, 0.9644420571358222]
stsb_roberta_validation_0_005 = [0.8651036468393238, 0.8702586974337877, 0.8653183640693531, 0.8646332799666762, 0.8718926141579146, 0.8638729627159943, 0.8690413310514238, 0.8696573190210077, 0.8717055677654867, 0.8713922846311374, 0.8733942824540185, 0.8695558741111417, 0.8696916802546851, 0.8715404875803888, 0.8728909760568593, 0.8724223013561978, 0.8714429572639717, 0.8722341944648174, 0.8718416675035388, 0.8721799275234526]
stsb_roberta_test_0_005 = 0.846493724299033
stsb_roberta_train_0_0005 = [0.959692224990891, 0.9420935623451908, 0.9324909760217605, 0.9410351177468553, 0.9410436329171102, 0.9443021883653437, 0.9479504806880777, 0.9500841561079514, 0.9524131876191133, 0.9548882787277755, 0.9566024265730534, 0.957936970597392, 0.958865694600129, 0.9597329104780798, 0.9604873081311582, 0.9626608309231238, 0.9627804688525319, 0.9634002391865304, 0.9631947945659333, 0.9635471413792023]
stsb_roberta_validation_0_0005 = [0.86739611105483, 0.8658101348713628, 0.8676079670729201, 0.8626591255793801, 0.8691182047699557, 0.8662327721737055, 0.8676364404021785, 0.8678786309607757, 0.8708832931766739, 0.8713403379760714, 0.8672325255326853, 0.8672369810517742, 0.8679750420593387, 0.8670209773439889, 0.8666110535735516, 0.8678875829242491, 0.8672365753575441, 0.8677702038396858, 0.8670588083309505, 0.8673557195678806]
stsb_roberta_test_0_0005 = 0.8421929999390478


x = range(1, len(nli_distilroberta_train_0_0005)+1)
# scalling from <0,1> to range <0, 100>
nli_distilroberta_train_0_0005 = [i*100 for i in nli_distilroberta_train_0_0005]
nli_distilroberta_validation_0_0005 = [i*100 for i in nli_distilroberta_validation_0_0005]
nli_distilroberta_train_0_0001 = [i*100 for i in nli_distilroberta_train_0_0001]
nli_distilroberta_validation_0_0001 = [i*100 for i in nli_distilroberta_validation_0_0001]
stsb_roberta_train_0_005 = [i*100 for i in stsb_roberta_train_0_005]
stsb_roberta_validation_0_005 = [i*100 for i in stsb_roberta_validation_0_005]
stsb_roberta_train_0_0005 = [i*100 for i in stsb_roberta_train_0_0005]
stsb_roberta_validation_0_0005 = [i*100 for i in stsb_roberta_validation_0_0005]

plt.plot( x, nli_distilroberta_train_0_0005, '-o', label=r'NLI-DistilRoBERTa-base-v2, $\lambda$ = 0.0005, tren.', lw=1.1, ms=4 , c='#FF9A49' )
plt.plot( x, nli_distilroberta_validation_0_0005, '-D', label=r'NLI-DistilRoBERTa-base-v2, $\lambda$ = 0.0005, wal.', lw=0.8, ms=3.2, c='#FFCC66' )
plt.plot( x, nli_distilroberta_train_0_0001, '-o', label=r'NLI-DistilRoBERTa-base-v2, $\lambda$ = 0.0001, tren.', lw=1.1, ms=4 , c='#D21E05' )
plt.plot( x, nli_distilroberta_validation_0_0001, '-D', label=r'NLI-DistilRoBERTa-base-v2, $\lambda$ = 0.0001, wal.', lw=0.8, ms=3.2, c='#F39E9E' )
plt.plot( x, stsb_roberta_train_0_005, '-o', label=r'STS-B-RoBERTa-base-v2, $\lambda$ = 0.005, tren.', lw=1.1, ms=4 , c='#1F44A3' )
plt.plot( x, stsb_roberta_validation_0_005, '-D', label=r'STS-B-RoBERTa-base-v2, $\lambda$ = 0.005, wal.', lw=0.8, ms=3.2, c='#79C1E8' )
plt.plot( x, stsb_roberta_train_0_0005, '-o', label=r'STS-B-RoBERTa-base-v2, $\lambda$ = 0.0005, tren.', lw=1.1, ms=4 , c='#188977' )
plt.plot( x, stsb_roberta_validation_0_0005, '-D', label=r'STS-B-RoBERTa-base-v2, $\lambda$ = 0.0005, wal.', lw=0.8, ms=3.2, c='#6FC486' )


########################################################################
# Polski
########################################################################
# # sdadas/polish-roberta-base-v1
# pl_roberta_base_train_0_005 = [0.8696059801787841, 0.840017138925237, 0.8606307320408298, 0.8729929632242461, 0.8854493592610716, 0.8946653125353785, 0.901880897143569, 0.9079081443009858, 0.913667896573839, 0.9169249336587807, 0.9192281218873644, 0.9212062448716795, 0.924678607463578, 0.9248732774201852, 0.9271425895507363, 0.9280114376506342, 0.9296589456464082, 0.9301214098285072, 0.9304276490390029, 0.9303583628569854]
# pl_roberta_base_validation_0_005 = [0.8356823138806515, 0.8314464264624392, 0.8313249285159989, 0.842917606333311, 0.8534321467780633, 0.8574120168955983, 0.8600174363617149, 0.8637948059915166, 0.8687125224763512, 0.8668475284119416, 0.8689659941504594, 0.8681504317064672, 0.8676976621070837, 0.8703196222804013, 0.8698859038217943, 0.8725880453665437, 0.8724330906137648, 0.8745042217752925, 0.8749007881631863, 0.8747836072240156]
# pl_roberta_base_test_0_005 = 0.8776628866801239
# pl_roberta_base_train_0_0005 = [0.8452834643933647, 0.8461924674497335, 0.8499864051361248, 0.8706726362125031, 0.8812545780254698, 0.8887822755192895, 0.8986462212588785, 0.8976570284514559, 0.906973375587917, 0.9126893666076625, 0.9163334767999517, 0.9146183615410466, 0.9191425716405606, 0.9208878662932762, 0.9235039184933665, 0.923049530450298, 0.924892120372931, 0.9260626129517193, 0.9264791978991769, 0.9264960917851924]
# pl_roberta_base_validation_0_0005 = [0.8013563344372193, 0.8210675667407272, 0.8395581760235812, 0.8463099880702596, 0.8481752284856277, 0.8462351677649206, 0.856975013757547, 0.8608844392320025, 0.8592519770102441, 0.8633545173672479, 0.8626593744246518, 0.8632675562857366, 0.866022240970595, 0.8641747730172195, 0.870744366525527, 0.8688278616488339, 0.8691916985522662, 0.8686748307797294, 0.8697799377167256, 0.8701080185382084]
# pl_roberta_base_test_0_0005 = 0.8496155743623152
# # sdadas/polish-roberta-large-v2
# pl_roberta_large_train_0_0005 = [0.8555643467106803, 0.8653471277630006, 0.8681350942178269, 0.8834507187142643, 0.8977742225781838, 0.8989552638082958, 0.9066472707248335, 0.9124956577772357, 0.919862963701523, 0.9239347905782012, 0.9231253486656209, 0.9270500218862848, 0.9289450045608167, 0.9301152402841737, 0.9321627689511794, 0.9339652684099581, 0.935181280572598, 0.9348692922164025, 0.9357846292434548, 0.9362419059835525]
# pl_roberta_large_validation_0_0005 = [0.8485598527175422, 0.8486062018906992, 0.8676012215724032, 0.8557891154372222, 0.8746009321229274, 0.8749403802814867, 0.8734843405388746, 0.8720605493098055, 0.8792310983844207, 0.8776827708804253, 0.8809169953021369, 0.8792978243011295, 0.8789053989555348, 0.8816831115894963, 0.8816564822240025, 0.8818964632503934, 0.8837932483194235, 0.8841712797304538, 0.8842402228057875, 0.8851788316879582]
# pl_roberta_large_test_0_0005 = 0.8715068483940744
# pl_roberta_large_train_0_0001 = [0.8275024965824114, 0.8819306618306901, 0.88034791913727, 0.8795009448891652, 0.8960623045245287, 0.8947039771880648, 0.9105805598586718, 0.9177319986169483, 0.9209305039068756, 0.9219901417173832, 0.9250691265812625, 0.9268737609215929, 0.9288746283352165, 0.9293699750226578, 0.9284216769388853, 0.9297578313049818, 0.9298484814921077, 0.9321282810775817, 0.9325814456964928, 0.932557525684919]
# pl_roberta_large_validation_0_0001 = [0.8616876137420911, 0.8355492491701685, 0.8469971430099806, 0.8594556975217712, 0.8769527274119618, 0.870761740133587, 0.8714356390455513, 0.8756170007849904, 0.8804837512425429, 0.8853060074374378, 0.8813903645340344, 0.8776846478401078, 0.878439619679353, 0.8792634642079439, 0.8825810256395532, 0.8816704655736363, 0.8814196802980735, 0.8819223652940104, 0.8826246297591746, 0.8830391563050294]
# pl_roberta_large_test_0_0001 = 0.8758259698011304
#
#
# x = range(1, len(pl_roberta_base_train_0_005)+1)
# # scalling from <0,1> to range <0, 100>
# pl_roberta_base_train_0_005 = [i*100 for i in pl_roberta_base_train_0_005]
# pl_roberta_base_validation_0_005 = [i*100 for i in pl_roberta_base_validation_0_005]
# pl_roberta_base_train_0_0005 = [i*100 for i in pl_roberta_base_train_0_0005]
# pl_roberta_base_validation_0_0005 = [i*100 for i in pl_roberta_base_validation_0_0005]
# pl_roberta_large_train_0_0005 = [i*100 for i in pl_roberta_large_train_0_0005]
# pl_roberta_large_validation_0_0005 = [i*100 for i in pl_roberta_large_validation_0_0005]
# pl_roberta_large_train_0_0001 = [i*100 for i in pl_roberta_large_train_0_0001]
# pl_roberta_large_validation_0_0001 = [i*100 for i in pl_roberta_large_validation_0_0001]
#
# plt.plot( x, pl_roberta_base_train_0_005, '-o', label=r'Polish-RoBERTa-base-v1, $\lambda$ = 0.005, tren.', lw=1.1, ms=4 , c='#D21E05' )
# plt.plot( x, pl_roberta_base_validation_0_005, '-D', label=r'Polish-RoBERTa-base-v1, $\lambda$ = 0.005, wal.', lw=0.8, ms=3.2, c='#F39E9E' )
# plt.plot( x, pl_roberta_base_train_0_0005, '-o', label=r'Polish-RoBERTa-base-v1, $\lambda$ = 0.0005, tren.', lw=1.1, ms=4 , c='#FF9A49' )
# plt.plot( x, pl_roberta_base_validation_0_0005, '-D', label=r'Polish-RoBERTa-base-v1, $\lambda$ = 0.0005, wal.', lw=0.8, ms=3.2, c='#FFCC66' )
# plt.plot( x, pl_roberta_large_train_0_0005, '-o', label=r'Polish-RoBERTa-large-v2, $\lambda$ = 0.0005, tren.', lw=1.1, ms=4 , c='#1F44A3' )
# plt.plot( x, pl_roberta_large_validation_0_0005, '-D', label=r'Polish-RoBERTa-large-v2, $\lambda$ = 0.0005, wal.', lw=0.8, ms=3.2, c='#79C1E8' )
# plt.plot( x, pl_roberta_large_train_0_0001, '-o', label=r'Polish-RoBERTa-large-v2, $\lambda$ = 0.0001, tren.', lw=1.1, ms=4 , c='#188977' )
# plt.plot( x, pl_roberta_large_validation_0_0001, '-D', label=r'Polish-RoBERTa-large-v2, $\lambda$ = 0.0001, wal.', lw=0.8, ms=3.2, c='#6FC486' )



title = 'Liczba epok: 20'

ax = plt.gca()
ax.xaxis.set_minor_locator( MultipleLocator(1) )
ax.yaxis.set_minor_locator( MultipleLocator(.5) )

plt.xlabel( 'Liczba epok', labelpad=10 )
plt.xticks( x )
plt.ylabel( r'$\rho$ Spearmana', labelpad=20 )
plt.title( title, pad=10 )
plt.legend( loc='best' )
    
plt.show()

